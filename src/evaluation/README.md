# Evaluation Framework

This framework assesses the performance and correctness of the multi-agent workflow using GPT-4 as a judge.

## Running Evaluations

From the project root directory, run:

```bash
python -m src.evaluation.run_evaluation
```

## What Gets Evaluated

1. **Task Completion**
   - Whether all requested tasks were completed correctly
   - If tasks were executed in logical order
   - Quality of task execution

2. **Node Execution**
   - Whether the right agents were involved
   - If agent interactions made sense
   - Efficiency of the workflow

## Example Output
```
üöÄ Starting Evaluation Process
==============================

1Ô∏è‚É£ Setting up test dataset...
‚úì Dataset ready with test case: Generate image with text overlay

[... evaluation steps ...]

8Ô∏è‚É£ Quick Summary
===============
‚Ä¢ Request: Generate an image of a sunset and add text 'Beautiful Evening'
‚Ä¢ Task Completion Score: 1.0
‚Ä¢ Node Execution Score: 1.0
‚Ä¢ Execution Time: 2.27 seconds
```

## Components

1. **Test Dataset** (`create_dataset.py`)
   - Predefined test cases with expected outcomes
   - Stored in LangSmith for tracking

2. **Evaluators** (`evaluators.py`)
   - GPT-4 powered evaluation functions
   - Task completion checker
   - Node execution analyzer

3. **Runner** (`run_evaluation.py`)
   - Main evaluation script
   - Results processing and display
   - LangSmith integration

## Project Structure
```
evaluation/
‚îú‚îÄ‚îÄ README.md           # This file
‚îú‚îÄ‚îÄ evaluators.py       # Evaluation functions
‚îú‚îÄ‚îÄ create_dataset.py   # Test dataset creation
‚îî‚îÄ‚îÄ run_evaluation.py   # Main evaluation script
``` 